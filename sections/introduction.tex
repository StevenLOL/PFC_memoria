\section{Introduction}
\label{intro}
\thispagestyle{empty}

Speech synthesis is not a recent ambition in mankind history. The earliest attempts to synthesize speech are only legends starring Gerbert d'Aurillac (died 1003 A.D.), also known as Pope Sylvester II. The pretended system used by him was a brazen head: a legendary automaton imitating the anatomy of a human head and capable to answer any question. Back in those days, the brazen heads were said to be owned by wizards. Following Pope Sylvester II, some important characters in mankind history  were reputed to have one of these heads, such as Albertus Magnus or Roger Bacon.

During the 18th century, Christian Kratzenstein, a German-born doctor, physicist and engineer working at the Russian Academy of Sciences, was able to built acoustics resonators similar to the human vocal tract. He activated the resonators with vibrating reeds producing the the five long vowels: /a/, /e/, /i/, /o/ and /u/.

Almost at the end of the 18th century, in 1791, Wolfgang von Kempelen presented his Acoustic-Mechanical Speech Machine \cite{vonKempelen}, which was able to produce single sounds and some combinations. During the first half of the 19th century, Charles Wheatstone built his improved and more complicated version of Kempelen's Acoustic-Mechanical Speech Machine, capable of producing vowels, almost all the consonants, sound combinations and even some words.	

In the late 1800's, Alexander Graham Bell also built a speaking machine and did some questionable experiments changing with his hands the vocal tract of his dog and making the dog bark in order to produce speech-like sounds \cite{Schroeder93, LemmettyMSc}.

Before World War II, Bell labs developed the vocoder, which analyzed and extract fundamentals tone and frequency from speech. In the 1950's, the first computer based speech synthesis systems were created and in 1968 the first general English text-to-speech (TTS) system was developed at the Electrotechnical Laboratory, Japan \cite{Klatt87}. From that time on, the main branch of speech synthesis development has being focused on electronic systems, but research conducted on mechanical synthesizers has not been abandoned \cite{mechSynthWeb, mechSynth}.

All the different kind of systems named pursued the same goal: produce natural sounding speech, which is the main goal of speech synthesis. As an extra requirement to this main goal, TTS systems aim to create the speech from arbitrary texts given as inputs, increasing the difficulty. It is easy to assume that a considerably amount of data is needed in order to cover all the possible sounds combinations in a given text. Moreover, the current trend in TTS aims towards generating different speaking styles with different speaker characteristics and emotions expressed with our voice, enlarging the spectrum of the characteristics of the voice to take into account and its differences depending on the context, increasing the amount of data needed to develop the final system. 

It must be pointed out that among all the different techniques used nowadays to synthesize speech, someones are not focused in maximum naturalness but intelligibility or high-speed synthesized speech. Although naturalness still a main issue, the final target, e.g. helping impaired people to navigate computers using a screen reader, forces to prioritize some other characteristics before naturalness. 

Among the synthesis techniques, when talking about fulfilling the general requirements presented so far: naturalness, speaker characteristics, emotions, style, etc., unit selection technique and Hidden Markov Model (HMM) approach stand out. Although unit selection synthesis provides the greatest naturalness, it does not allow an easy adaptation of a TTS system to other speakers or speaking styles, requiring a large amount of data due to the selection and concatenation used in this kind of synthesis, making this technique not suitable for example to embedded systems. On the other hand, HMM-based systems make easier to use adaptation techniques and require less memory, unfortunately meaning a reduction in naturalness.

HMM-based speech synthesis is very popular nowadays thanks to the advantages previously commented. We can find various vocoders currently being used in HMM-based systems, but the Speech Transformation and Representation using Adaptive Interpolation of weiGHT spectrum (STRAIGHT) vocoder is the most commonly used and the most established one. However, due to the degradation in naturalness suffered in HMM-based systems, a new vocoder is being developed trying to solve this issue: the GlottHMM vocoder, which estimates the real glottal signal and the real vocal tract associated to it, thus producing a more natural voice. 

So far memory requirements and the amount of data needed to build the system have been pointed as some of the weak points in speech synthesis systems. The amount of data is particularly important in HMM-based systems, as their performance is largely dictated by the amount and quality of the training data. Sadly, collecting data is not an easy task since speech synthesis systems need high quality recordings covering different contexts. Moreover, when using speaker adaptive systems certain amount of audio recordings will be needed from a substantial number of speakers. Adapting an average voice model, made out from high quality recorded audio of different speakers, with non high quality recordings would facilitate the access to a bigger number of target voices.

Noisy conditions were explored in speech recognition systems before being tested in synthesis system. Speech recognition is obviously highly related to speech synthesis, e.g. the analysis done to the audio recordings is the same in both cases, thus the same concepts used in recognition can be applied to speech synthesis systems. Nevertheless, speech recognition techniques under noisy conditions cannot satisfy all the needs of speech synthesis, so further research is being and must me done in the future.

In this project the possibility of synthesizing speech from noisy data will be explored. The aim is to adapt an average voice model made from high-quality training data, recorded in studio conditions, with noisy data, which is easier to obtain. HMM-based speech paradigm has been found to be quite robust on Mel-Cepstrum \cite{karhila_jstsp_14, yamagishi2008robustness} and Mel-LSP-based vocoders \cite{Yanagisawa_SSW8}, but different adaptation techniques, vocoding techniques and noise present in the adaptation data can reduce quality, naturalness and speaker similarity and also add some background noise to the synthesized speech compared to the adaptation made from clean data. 

A similar approach to this problem has been carried out in \cite{karhila_jstsp_14} using STRAIGHT vocoder. As GlottHMM targets on obtaining more natural voices, in this project we will study the effects of different types of noise present in adaptation data, using objective measures and subjective tests to evaluate the results. Besides, we will compare the performance made by GlottHMM vocoder with the one made by STRAIGHT vocoder in \cite{karhila_jstsp_14}, trying to established which conditions benefit each vocoder against the other and learn about the level of acceptance of the synthesized voices observed in the subjective tests. To make the comparison as fair as possible, we will be working in Finnish with the same training and adaptation data.
